{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Different Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"color:blue;font-family:Candara,arial,helvetica;line-height:20px\"><strong>\n",
    "\n",
    "\n",
    "### True positive and true negatives are the observations that are correctly predicted and therefore shown in green. We want to minimize false positives and false negatives so they are shown in red color. These terms are a bit confusing. So let’s take each term one by one and understand it fully.\n",
    "\n",
    "## True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.\n",
    "\n",
    "## True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.\n",
    "\n",
    "### False positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n",
    "\n",
    "## False Positives (FP) – When actual class is no and predicted class is yes. E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.\n",
    "\n",
    "## False Negatives (FN) – When actual class is yes but predicted class in no. E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.\n",
    "\n",
    "### Once you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n",
    "\n",
    "## Accuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. For our model, we have got 0.803 which means our model is approx. 80% accurate.\n",
    "\n",
    "## Accuracy = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "## Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. We have got 0.788 precision which is pretty good.\n",
    "\n",
    "## Precision = TP/TP+FP\n",
    "\n",
    "## Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label? We have got recall of 0.631 which is good for this model as it’s above 0.5.\n",
    "\n",
    "## Recall = TP/TP+FN\n",
    "\n",
    "## F1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall. In our case, F1 score is 0.701.\n",
    "\n",
    "## F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "    \n",
    "    \n",
    "<img src=\"https://i.stack.imgur.com/U0hjG.pngg\" alt=\"drawing\" width=\"600\" height=\"300\"/>     \n",
    "    \n",
    "<img src=\"https://blog.exsilio.com/wp-content/uploads/2016/09/table-blog.png\" alt=\"drawing\" width=\"600\" height=\"300\"/>     \n",
    "   \n",
    "\n",
    "</strong></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Classification Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries,Preprocess data,Split Data to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data and Create a copy\n",
    "LoanData = pd.read_csv(\"01Exercise1.csv\")\n",
    "LoanPrep = LoanData.copy()\n",
    "\n",
    "\n",
    "#find out columns with missing values\n",
    "LoanPrep.isnull().sum(axis=0)\n",
    "\n",
    "\n",
    "# Replace Missing Values. Drop the rows.\n",
    "LoanPrep = LoanPrep.dropna()\n",
    "\n",
    "# Drop irrelevant columns based on business sense\n",
    "LoanPrep = LoanPrep.drop(['gender'], axis=1)\n",
    "\n",
    "# Create Dummy variables\n",
    "LoanPrep.dtypes\n",
    "LoanPrep = pd.get_dummies(LoanPrep, drop_first=True)\n",
    "\n",
    "\n",
    "# Normalize the data (Income and Loan Amount) Using StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scalar_ = StandardScaler()\n",
    "\n",
    "LoanPrep['income'] = scalar_.fit_transform(LoanPrep[['income']])\n",
    "LoanPrep['loanamt'] = scalar_.fit_transform(LoanPrep[['loanamt']])\n",
    "\n",
    "\n",
    "# Create the X (Independent) and Y (Dependent) dataframes\n",
    "# -------------------------------------------------------\n",
    "Y = LoanPrep[['status_Y']]\n",
    "X = LoanPrep.drop(['status_Y'], axis=1)\n",
    "\n",
    "\n",
    "# Split the X and Y dataset into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = \\\n",
    "train_test_split(X, Y, test_size = 0.3, random_state = 1234, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model and Get Accuracy,Precision,Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 29  20]\n",
      " [  2 108]]\n",
      "0.8616352201257862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.59      0.72        49\n",
      "           1       0.84      0.98      0.91       110\n",
      "\n",
      "    accuracy                           0.86       159\n",
      "   macro avg       0.89      0.79      0.82       159\n",
      "weighted avg       0.87      0.86      0.85       159\n",
      "\n",
      "0.8616352201257862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# Build the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "\n",
    "# Predict the outcome using Test data\n",
    "Y_predict = lr.predict(X_test)\n",
    "\n",
    "# import libraries to evaluate the model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Build Confusion Matrix, score and report for the default model\n",
    "cm1 = confusion_matrix(Y_test, Y_predict)\n",
    "score1 = lr.score(X_test, Y_test)\n",
    "cr1 = classification_report(Y_test, Y_predict)\n",
    "accscore1=accuracy_score(Y_test, Y_predict)\n",
    "\n",
    "print(cm1)\n",
    "print(score1)\n",
    "print(cr1)\n",
    "print(accscore1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting Thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div  style=\"color:blue;font-family:Candara,arial,helvetica;line-height:20px\"><strong>\n",
    "\n",
    "### The decision threshold creates a trade-off between the number of positives that you predict and the number of negatives that you predict -- because, tautologically, increasing the decision threshold will decrease the number of positives that you predict and increase the number of negatives that you predict.\n",
    "    \n",
    "</strong></div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  1]\n",
      " [70 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.98      0.57        49\n",
      "           1       0.98      0.36      0.53       110\n",
      "\n",
      "    accuracy                           0.55       159\n",
      "   macro avg       0.69      0.67      0.55       159\n",
      "weighted avg       0.80      0.55      0.54       159\n",
      "\n",
      "0.5534591194968553\n"
     ]
    }
   ],
   "source": [
    "# Create prediction probability list\n",
    "Y_prob = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create new predictions based on new probability threshold\n",
    "Y_new_pred = []\n",
    "threshold  = 0.8\n",
    "\n",
    "for i in range(0, len(Y_prob)):\n",
    "    if Y_prob[i] > threshold:\n",
    "        Y_new_pred.append(1)\n",
    "    else:\n",
    "        Y_new_pred.append(0)\n",
    "        \n",
    "# Check the effect of probability threshold on predictions\n",
    "cm2 = confusion_matrix(Y_test, Y_new_pred)\n",
    "#score2 = lr.score(X_test, Y_test)\n",
    "cr2 = classification_report(Y_test, Y_new_pred)\n",
    "accscore2 = accuracy_score(Y_test, Y_new_pred)\n",
    "\n",
    "\n",
    "print(cm2)\n",
    "#print(score1)\n",
    "print(cr2)\n",
    "print(accscore2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is AUC - ROC Curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div  style=\"color:blue;font-family:Candara,arial,helvetica;line-height:20px\"><strong>\n",
    "\n",
    "\n",
    "## AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n",
    "## The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.    \n",
    "    \n",
    "<img src=\"https://miro.medium.com/max/451/1*pk05QGzoWhCgRiiFbz-oKQ.png\" alt=\"drawing\" width=\"600\" height=\"300\"/>     \n",
    "    \n",
    "  \n",
    "   \n",
    "## Defining terms used in AUC and ROC Curve.\n",
    "### TPR (True Positive Rate) / Recall /Sensitivity\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/444/1*HgxNKuUwXk9JHYBCt_KZNw.png\" alt=\"drawing\" width=\"600\" height=\"300\"/>       \n",
    "    \n",
    "\n",
    "### Specificity\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/308/1*f7NmMcQtfes1ng7jtjNtHQ.png\" alt=\"drawing\" width=\"600\" height=\"300\"/>   \n",
    "\n",
    "### FPR\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/306/1*3GhDfiuhvINF5-9eL8g6Pw.png\" alt=\"drawing\" width=\"600\" height=\"300\"/>  \n",
    "    \n",
    "</strong></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
